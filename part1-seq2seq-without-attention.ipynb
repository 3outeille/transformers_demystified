{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO:\n",
    "    - Add color when there is sentence in `== ... ==`\n",
    "    - Fix image dimension according to google chrome/github\n",
    "    - Fix image display on github (check reserch-paper-summary repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ Seq2Seq without attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Source: \n",
    ">   - [Language Modeling - Lena Volta](https://lena-voita.github.io/nlp_course/language_modeling)\n",
    ">   - [Seq2Seq & Attention - Lena Volta](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "\n",
    "- Let's first understand how to model language using Neural Network. Intuitively, neural Language Models needs do two things:\n",
    "    - **process context**: The main idea here is to get a vector representation of the input. This part could be different depending on model architecture (e.g., RNN, CNN, whatever you want), but the main point is to encode context.\n",
    "    - **generate a probability distribution for the next token**: Once a context has been encoded (vector representation), a model uses it to predict a probability distribution for the next token - see below.\n",
    "    <p align=\"center\"> <img src=\"./assets/language_modeling.png\" height=\"500\" width=\"1100\" /></p> \n",
    "\n",
    "- In order to transform our context vector (which suppose to represent the input `I saw a cat on a`) into a probability distribution, we use a linear layer as projection.\n",
    "<p align=\"center\"> <img src=\"./assets/linear_layer_projection.png\" height=\"300\" width=\"1200\" /></p> \n",
    "\n",
    "-  If we zoom a little bit into the linear layer, we noticed that it is performing a matrix multiplication (vectorized dot product !). Thus, what we are actually doing is propagating/weighting the context vector to each class.\n",
    "<table><tr>\n",
    "    <td><img src=\"./assets/linear_layer_zoom.png\" height=\"350\" width=\"800\" /></td>\n",
    "    <td><img src=\"./assets/linear_layer_viz.png\" height=\"350\" width=\"1200\" /></td>\n",
    "</tr></table>\n",
    "\n",
    "- Neural classifiers are trained to predict probability distributions over classes. Intuitively, at each step we maximize the probability a model assigns to the correct class using a cross entropy loss.\n",
    "- For each input sentence (i.e `The cat is the worst <eos>`), we pass it to the NN to get context vector which is then pass to linear layer to get a probability vector (using softmax). \n",
    "- The cross entropy loss will be used to compare probability vector with the correct label (represented as one-hot-encoding vector). It will we maximize the probability a model assigns to the correct class and at the same time, minimize sum of the probabilities of incorrect classes (**since sum of all probabilities is constant, by increasing one probability we decrease sum of all the rest**)\n",
    "\n",
    "<p align=\"center\"> <video controls src=\"./assets/rnn_lm_training_with_target.mp4\" height=\"350\" width=\"800\" /> </p>\n",
    "\n",
    "---\n",
    "- The \"Neural Network\" is in fact an Encoder-Decoder architecture\n",
    "- Encoder-decoder is the standard modeling paradigm for sequence-to-sequence tasks which consists of 2 components:\n",
    "    - **Encoder**: Processes the input sequence and compresses the information into a **context vector/sentence embedding/\"thought\" vector** of a fixed length which is ==expected to be a good summary of the meaning of the whole source sequence.==\n",
    "    - **Decoder**: Use the last state of the encoder network as the decoder initial state to emit the transformed output. <p align=\"center\"> <img src=\"./assets/encoder-decoder.png\" height=\"400\" width=\"550\" /></p> \n",
    "\n",
    "- More precisely, we want to predict a probability distribution for the next token from the decoder context vector.\n",
    "    <p align=\"center\"> <img src=\"./assets/encoder-decoder-detailed.png\" height=\"400\" width=\"600\" /></p> \n",
    "\n",
    "> - The notation `P(* | I saw a cat, <russian sentence>)` is quite weird, lets demystify it:\n",
    ">   - We train the model to predict probability distributions of the next token based on \"source tokens\" and \"previous target tokens\".\n",
    "> - Thus, `P(* | I saw a cat, <russian sentence>)` is the **probability to get the next word knowing** `I saw a cat` **and** `<russian sentence>`\n",
    "> - In the image below, `P(*| I saw a, <russian sentence>)` => **probability to get the next word knowing** `I saw a` **and** `<russian sentence>`\n",
    "> <p align=\"center\"> <img src=\"./assets/encoder-decoder-russian.png\" height=\"400\" width=\"800\" /></p> \n",
    "\n",
    "- The whole process looks like this:  <p align=\"center\"> <video controls src=\"./assets/seq2seq_training_with_target.mp4\" height=\"350\" width=\"800\" /> </p>\n",
    "\n",
    "---\n",
    "\n",
    "- Now let's define **Seq2Seq**:\n",
    "    - **Goal**: aims to transform an input (source) sequence to a new one (target sequence). Both sequences can be of arbitrary lengths.\n",
    "    - **Structure**: encoder-decoder architecture composed of:\n",
    "        - **Encoder**: Processes the input sequence and compresses the information into a **context vector/sentence embedding/\"thought\" vector** of a fixed length which is ==expected to be a good summary of the meaning of the whole source sequence.==\n",
    "        - **Decoder**: Use the last state of the encoder network as the decoder initial state to emit the transformed output.\n",
    "        - Both the encoder and decoder are RNN (i.e. using LSTM or GRU units) or CNN\n",
    "        <p align=\"center\"> <img src=\"./assets/rnn-context-vector-lily.png\" height=\"400\" width=\"800\" /></p> \n",
    "\n",
    "- Let's code a simple encoder and decoder (using RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Sampled 100 sentence pairs (for faster training)\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 92\n",
      "eng 62\n",
      "['je suis faineant .', 'i m lazy .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    pairs = pairs[:100]\n",
    "    print(\"Sampled %s sentence pairs (for faster training)\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    dataset = {}\n",
    "    dataset[\"input_lang\"] = input_lang\n",
    "    dataset[\"output_lang\"] = output_lang\n",
    "    dataset[\"pairs\"] = pairs\n",
    "    return dataset\n",
    "\n",
    "dataset = prepareData('eng', 'fra', True)\n",
    "print(random.choice(dataset[\"pairs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 (10.0%) Loss: 1.4665349586804708\n",
      "Step: 200 (20.0%) Loss: 0.9519444440205896\n",
      "Step: 300 (30.0%) Loss: 0.7524634140531221\n",
      "Step: 400 (40.0%) Loss: 0.6167872877915701\n",
      "Step: 500 (50.0%) Loss: 0.45494514699776945\n",
      "Step: 600 (60.0%) Loss: 0.34018826515475903\n",
      "Step: 700 (70.0%) Loss: 0.29401343563199056\n",
      "Step: 800 (80.0%) Loss: 0.20605764707426233\n",
      "Step: 900 (90.0%) Loss: 0.13313032945990558\n",
      "Step: 1000 (100.0%) Loss: 0.09674693930273254\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    last_layer_encoder_hidden_states_foreach_input = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "    loss = 0\n",
    "\n",
    "    # https://stackoverflow.com/a/48305882/8623609\n",
    "    # encoder_output: give you the hidden layer outputs of the network for each time-step, but only for the final layer (\"top\")\n",
    "    # encoder_hidden: give you the hidden layer outputs of the network for the last time-step only, but for all layers (\"last right column\")\n",
    "    for i in range(input_length):\n",
    "        last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "        last_layer_encoder_hidden_states_foreach_input[i] = last_layer_encoder_hidden_states.squeeze()\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "    # \"Teacher forcing\" trick for faster convergence\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]  # Teacher forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, dataset, n_iters, print_every=100):\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    training_pairs = [tensorsFromPair(dataset[\"input_lang\"], dataset[\"output_lang\"], random.choice(dataset[\"pairs\"]))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Step: {iter} ({iter / n_iters * 100}%) Loss: {print_loss_avg}\")\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(dataset[\"input_lang\"].n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, dataset[\"output_lang\"].n_words).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  je pars maintenant .\n",
      "Expected: i m going .\n",
      "Pred:  i m going . <EOS>\n",
      "\n",
      "Input:  je suis faineant .\n",
      "Expected: i am lazy .\n",
      "Pred:  i am lazy . <EOS>\n",
      "\n",
      "Input:  je suis triste .\n",
      "Expected: i m sad .\n",
      "Pred:  i m sad . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, dataset, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(dataset[\"input_lang\"], sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "\n",
    "        last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "\n",
    "        for i in range(input_length):\n",
    "            last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset[\"output_lang\"].index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words[:di + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, dataset, n=3):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(dataset[\"pairs\"])\n",
    "        print(\"Input: \", pair[0])\n",
    "        print(\"Expected:\", pair[1])\n",
    "        output_words = evaluate(encoder, decoder, dataset, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print(\"Pred: \", output_sentence)\n",
    "        print('')\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What's wrong with Seq2Seq ?** We are expecting the fixed-length context vector of the last encoder to be a good summarize of the whole input source is too much to ask. Often it has forgotten the first part once it completes processing the whole input.\n",
    "- ==**The attention mechanism was born (Bahdanau et al., 2015) to resolve this problem**==\n",
    "- Summary: <p align=\"center\"> <img src=\"assets/part1-summary.png\" height=\"400\" width=\"500\" /></p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e26828b124e3e550b24680f350232b9e34674b0e62607605e5758cc17fa49831"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
