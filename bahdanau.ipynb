{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Sampled 100 sentence pairs (for faster training)\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 92\n",
      "eng 62\n",
      "['je suis en forme .', 'i m fit .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    pairs = pairs[:100]\n",
    "    print(\"Sampled %s sentence pairs (for faster training)\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    dataset = {}\n",
    "    dataset[\"input_lang\"] = input_lang\n",
    "    dataset[\"output_lang\"] = output_lang\n",
    "    dataset[\"pairs\"] = pairs\n",
    "    return dataset\n",
    "\n",
    "dataset = prepareData('eng', 'fra', True)\n",
    "print(random.choice(dataset[\"pairs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # https://stackoverflow.com/a/48305882/8623609\n",
    "        # encoder_output: give you the hidden layer outputs of the network for each time-step, but only for the final layer (\"top\")\n",
    "        # encoder_hidden: give you the hidden layer outputs of the network for the last time-step only, but for all layers (\"last right column\")\n",
    "        last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = self.gru(embedded, hidden)\n",
    "        return last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderAttentionRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.attn_proj = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input):\n",
    "        embedded = self.embedding(decoder_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        x = torch.tanh(self.fc_hidden(decoder_hidden)+self.fc_encoder(last_layer_encoder_hidden_states_foreach_input))\n",
    "        alignment_scores = x.bmm(self.weight.unsqueeze(2))\n",
    "        attn_weights = F.softmax(alignment_scores.squeeze(2), dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0), last_layer_encoder_hidden_states_foreach_input.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded, context_vector), -1).squeeze(0)\n",
    "        output = self.attn_proj(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        last_layer_decoder_hidden_states, last_time_step_decoder_hidden_states = self.gru(output, decoder_hidden)\n",
    "        last_layer_decoder_hidden_states = F.log_softmax(self.out(last_layer_decoder_hidden_states[0]), dim=1)\n",
    "        return last_layer_decoder_hidden_states, last_time_step_decoder_hidden_states, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 (10.0%) Loss: 1.4960639825661983\n",
      "Step: 200 (20.0%) Loss: 0.9305914251406988\n",
      "Step: 300 (30.0%) Loss: 0.6985346862673761\n",
      "Step: 400 (40.0%) Loss: 0.583538605988026\n",
      "Step: 500 (50.0%) Loss: 0.30018163283665983\n",
      "Step: 600 (60.0%) Loss: 0.2737010692954064\n",
      "Step: 700 (70.0%) Loss: 0.2100257837921381\n",
      "Step: 800 (80.0%) Loss: 0.13894890621801215\n",
      "Step: 900 (90.0%) Loss: 0.1403068618948261\n",
      "Step: 1000 (100.0%) Loss: 0.10856539232035482\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    last_layer_encoder_hidden_states_foreach_input = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "    loss = 0\n",
    "\n",
    "    # https://stackoverflow.com/a/48305882/8623609\n",
    "    # encoder_output: give you the hidden layer outputs of the network for each time-step, but only for the final layer (\"top\")\n",
    "    # encoder_hidden: give you the hidden layer outputs of the network for the last time-step only, but for all layers (\"last right column\")\n",
    "    for i in range(input_length):\n",
    "        last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "        last_layer_encoder_hidden_states_foreach_input[i] = last_layer_encoder_hidden_states.squeeze()\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]  # Teacher forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, dataset, n_iters, print_every=100):\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    training_pairs = [tensorsFromPair(dataset[\"input_lang\"], dataset[\"output_lang\"], random.choice(dataset[\"pairs\"]))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Step: {iter} ({iter / n_iters * 100}%) Loss: {print_loss_avg}\")\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(dataset[\"input_lang\"].n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderAttentionRNN(hidden_size, dataset[\"output_lang\"].n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  je suis faineant .\n",
      "Expected: i m lazy .\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPred: \u001b[39m\u001b[39m\"\u001b[39m, output_sentence)\n\u001b[1;32m     45\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m evaluateRandomly(encoder1, attn_decoder1, dataset)\n",
      "Cell \u001b[0;32mIn [11], line 42\u001b[0m, in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, dataset, n)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m\"\u001b[39m, pair[\u001b[39m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExpected:\u001b[39m\u001b[39m\"\u001b[39m, pair[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 42\u001b[0m output_words, attentions \u001b[39m=\u001b[39m evaluate(encoder, decoder, dataset, pair[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     43\u001b[0m output_sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(output_words)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPred: \u001b[39m\u001b[39m\"\u001b[39m, output_sentence)\n",
      "Cell \u001b[0;32mIn [11], line 24\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, dataset, sentence, max_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(decoder_output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(decoder_attention\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 24\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m\n\u001b[1;32m     25\u001b[0m decoder_attentions[i] \u001b[39m=\u001b[39m decoder_attention\u001b[39m.\u001b[39mdata\n\u001b[1;32m     26\u001b[0m topv, topi \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtopk(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, dataset, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(dataset[\"input_lang\"], sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "        last_layer_encoder_hidden_states_foreach_input = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "            last_layer_encoder_hidden_states_foreach_input[i] = last_layer_encoder_hidden_states.squeeze()\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            print(decoder_output.shape)\n",
    "            print(decoder_attention.shape)\n",
    "            \n",
    "            raise Exception\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset[\"output_lang\"].index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:i + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, dataset, n=3):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(dataset[\"pairs\"])\n",
    "        print(\"Input: \", pair[0])\n",
    "        print(\"Expected:\", pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, dataset, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print(\"Pred: \", output_sentence)\n",
    "        print('')\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e26828b124e3e550b24680f350232b9e34674b0e62607605e5758cc17fa49831"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
