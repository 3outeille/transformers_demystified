{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Seq2seq with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Attention in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Source:\n",
    ">   - [Transformers from scratch - Peter Bloem](https://peterbloem.nl/blog/transformers)\n",
    "\n",
    "- Before talking about Bahdanau attention, let's understand what do people mean by attention. \n",
    "- Attention can be better explained through the lens of movie recommendation.\n",
    "- Let’s say you run a movie rental business and you have some movies, and some users, and you would like to recommend movies to your users that they are likely to enjoy.\n",
    "- One way to go about this, is to:\n",
    "    - create manual features for your movies: how much romance there is in the movie, and how much action\n",
    "    - create manual features for your users: how much they enjoy romantic movies and how much they enjoy action-based movies. \n",
    "- If you did this, the dot product between the two feature vectors would give you a score for how well the attributes of the movie match what the user enjoys.\n",
    "<p align=\"center\"> <img src=\"./assets/dot_product.svg\" height=\"500\" width=\"1100\" /></p> \n",
    "\n",
    "- If for example:\n",
    "    - the user enjoys romance and the movie has a lot of romance, then the dot product for that feature will be positive.\n",
    "    - the user hates romance and the movie has a lot of romance, then the dot product for that feature will be negative.\n",
    "- This is the basic intuition behind attention. The dot product helps us to represent relations between objects by expressing how related two vectors are.\n",
    "- How is dot product expressed in neural networks ? Through the use of matrix multiplication which is just a vectorized dot product !\n",
    "- However, there is a problem as matrix multiplication do not normalized the input ! As such, if we compute the similarity between `A` and `A.T`, we won't have a score of 1.0 in the diagonal as we would expect (because the similarity between oneself should be maximal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "[[0.375 0.951 0.732 0.599 0.156 0.156]\n",
      " [0.058 0.866 0.601 0.708 0.021 0.97 ]\n",
      " [0.832 0.212 0.182 0.183 0.304 0.525]\n",
      " [0.432 0.291 0.612 0.139 0.292 0.366]\n",
      " [0.456 0.785 0.2   0.514 0.592 0.046]\n",
      " [0.608 0.171 0.065 0.949 0.966 0.808]]\n",
      "--------------------\n",
      "norm of A = [1.265 1.559 1.161 1.441 1.219 1.425]\n",
      "Normalized A = \n",
      "[[0.296 0.61  0.63  0.416 0.128 0.109]\n",
      " [0.046 0.556 0.517 0.491 0.017 0.681]\n",
      " [0.658 0.136 0.157 0.127 0.249 0.368]\n",
      " [0.341 0.187 0.527 0.096 0.24  0.257]\n",
      " [0.36  0.504 0.172 0.357 0.486 0.032]\n",
      " [0.481 0.11  0.056 0.658 0.792 0.567]]\n",
      "--------------------\n",
      "Normalized dot product: \n",
      "[[1.    0.594 0.583 0.707 0.84  0.678]\n",
      " [0.594 1.    0.885 0.814 0.498 0.622]\n",
      " [0.583 0.885 1.    0.685 0.383 0.652]\n",
      " [0.707 0.814 0.685 1.    0.811 0.836]\n",
      " [0.84  0.498 0.383 0.811 1.    0.644]\n",
      " [0.678 0.622 0.652 0.836 0.644 1.   ]]\n",
      "Indices of maximum value = [0 1 2 3 4 5]\n",
      "--------------------\n",
      "Unormaliazed Dot product: \n",
      "[[1.6   1.171 0.856 1.289 1.296 1.222]\n",
      " [1.171 2.429 1.601 1.828 0.946 1.38 ]\n",
      " [0.856 1.601 1.349 1.147 0.542 1.078]\n",
      " [1.289 1.828 1.147 2.078 1.426 1.718]\n",
      " [1.296 0.946 0.542 1.426 1.486 1.119]\n",
      " [1.222 1.38  1.078 1.718 1.119 2.03 ]]\n",
      "Indices of maximum value = [0 1 1 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "A = np.array([\n",
    "    [0.375, 0.951, 0.732, 0.599, 0.156, 0.156],\n",
    "    [0.058, 0.866, 0.601, 0.708, 0.021, 0.97 ],\n",
    "    [0.832, 0.212, 0.182, 0.183, 0.304, 0.525],\n",
    "    [0.432, 0.291, 0.612, 0.139, 0.292, 0.366],\n",
    "    [0.456, 0.785, 0.2,   0.514, 0.592, 0.046],\n",
    "    [0.608, 0.171, 0.065, 0.949, 0.966, 0.808]\n",
    "])\n",
    "\n",
    "print(f\"A = \\n{A}\")\n",
    "print(\"--------------------\")\n",
    "# This means that when computing norm of A/N, it will be equal to 1\n",
    "n = np.linalg.norm(A, ord=2, axis=0)\n",
    "B = A / n\n",
    "\n",
    "print(f\"norm of A = {n}\")\n",
    "print(f\"Normalized A = \\n{B}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# If we compute the norm on axis=0 (columns) => features are on each column \n",
    "# => Transpose B to do matmul on the first feature \n",
    "print(f\"Normalized dot product: \\n{B.T @ B}\")\n",
    "# They are all in the diagonal because they are normalized\n",
    "print(f\"Indices of maximum value = {np.argmax(B.T @ B, axis=1)}\")\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(f\"Unormaliazed Dot product: \\n{A.T @ A}\")\n",
    "print(f\"Indices of maximum value = {np.argmax(A.T @ A, axis=1)}\")\n",
    "\n",
    "np.set_printoptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, the matrix multiplication is not properly reflecting the notion of “similarity”. One reason could be that matrix multiplication can be easily parallelized, engineers may have favor speed instead of “similarity precision” ? (maybe normalizing gives extra overhead ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Bahdanau attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"> <img src=\"./assets/lily-bahdanau.png\" height=\"500\" width=\"900\" /></p> \n",
    "\n",
    "- Attention mechanism (Bahdanau):\n",
    "    - **Goal**: help memorize long source sentences in neural machine translation (NMT).\n",
    "    - **Structure**: At different steps, let a model \"focus\" on different parts of the input. At each decoder step, it decides which source parts are more important. In this setting, the encoder does not have to compress the whole source into a single vector - it gives representations for all source tokens (for example, all RNN states instead of the last one).\n",
    "- The whole process looks like this:\n",
    "    - **Decoder `Hidden layer Nth`**:\n",
    "        - Init hidden state with last encoder output\n",
    "        - Compute **attention score**: use all encoder hidden states and decoder `hidden layer 1` state\n",
    "        - Compute **attention weights**: apply softmax to attention score\n",
    "        - Compute **attention output**: weighted sum between attention weights and all encoder states\n",
    "        - Pass **attention output** and **`decoder hidden state Nth`** to compute get **`decoder hidden state Nth+1`** (i.e `self.lstm(attention_output, hidden_nth)`)\n",
    "    <p align=\"center\"> <img src=\"./assets/bahdanau.png\" height=\"500\" width=\"900\" /></p>\n",
    "- So we can see that Bahdanau computes the score through a 1 single layer feed forward neural network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Sampled 100 sentence pairs (for faster training)\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 92\n",
      "eng 62\n",
      "['je suis certain .', 'i am sure .']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from attention_utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    pairs = pairs[:100]\n",
    "    print(\"Sampled %s sentence pairs (for faster training)\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    dataset = {}\n",
    "    dataset[\"input_lang\"] = input_lang\n",
    "    dataset[\"output_lang\"] = output_lang\n",
    "    dataset[\"pairs\"] = pairs\n",
    "    return dataset\n",
    "\n",
    "dataset = prepareData('eng', 'fra', True)\n",
    "print(random.choice(dataset[\"pairs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # https://stackoverflow.com/a/48305882/8623609\n",
    "        # encoder_output: give you the hidden layer outputs of the network for each time-step, but only for the final layer (\"top\")\n",
    "        # encoder_hidden: give you the hidden layer outputs of the network for the last time-step only, but for all layers (\"last right column\")\n",
    "        last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = self.gru(embedded, hidden)\n",
    "        return last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderAttentionRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(DecoderAttentionRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.attn_proj = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input):\n",
    "        embedded = self.embedding(decoder_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        x = torch.tanh(self.fc_hidden(decoder_hidden)+self.fc_encoder(last_layer_encoder_hidden_states_foreach_input))\n",
    "        alignment_scores = x.bmm(self.weight.unsqueeze(2))\n",
    "        attn_weights = F.softmax(alignment_scores.squeeze(2), dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0), last_layer_encoder_hidden_states_foreach_input.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded, context_vector), -1).squeeze(0)\n",
    "        output = self.attn_proj(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        last_layer_decoder_hidden_states, last_time_step_decoder_hidden_states = self.gru(output, decoder_hidden)\n",
    "        last_layer_decoder_hidden_states = F.log_softmax(self.out(last_layer_decoder_hidden_states[0]), dim=1)\n",
    "        return last_layer_decoder_hidden_states, last_time_step_decoder_hidden_states, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 (10.0%) Loss: 1.462863874832789\n",
      "Step: 200 (20.0%) Loss: 0.9862939871946975\n",
      "Step: 300 (30.0%) Loss: 0.69884103957812\n",
      "Step: 400 (40.0%) Loss: 0.5858553817470871\n",
      "Step: 500 (50.0%) Loss: 0.32183215591311454\n",
      "Step: 600 (60.0%) Loss: 0.23454589036107062\n",
      "Step: 700 (70.0%) Loss: 0.21993039281914625\n",
      "Step: 800 (80.0%) Loss: 0.13522455321003998\n",
      "Step: 900 (90.0%) Loss: 0.11338057126601536\n",
      "Step: 1000 (100.0%) Loss: 0.08471504014978805\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    last_layer_encoder_hidden_states_foreach_input = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "    loss = 0\n",
    "\n",
    "    # https://stackoverflow.com/a/48305882/8623609\n",
    "    # encoder_output: give you the hidden layer outputs of the network for each time-step, but only for the final layer (\"top\")\n",
    "    # encoder_hidden: give you the hidden layer outputs of the network for the last time-step only, but for all layers (\"last right column\")\n",
    "    for i in range(input_length):\n",
    "        last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "        last_layer_encoder_hidden_states_foreach_input[i] = last_layer_encoder_hidden_states.squeeze()\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]  # Teacher forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[i])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, dataset, n_iters, print_every=100):\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    training_pairs = [tensorsFromPair(dataset[\"input_lang\"], dataset[\"output_lang\"], random.choice(dataset[\"pairs\"]))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Step: {iter} ({iter / n_iters * 100}%) Loss: {print_loss_avg}\")\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(dataset[\"input_lang\"].n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderAttentionRNN(hidden_size, dataset[\"output_lang\"].n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  il est mouille .\n",
      "Expected: he s wet .\n",
      "Pred:  he s wet . <EOS>\n",
      "\n",
      "Input:  je suis tatillonne .\n",
      "Expected: i m fussy .\n",
      "Pred:  i m fussy . <EOS>\n",
      "\n",
      "Input:  je suis repu !\n",
      "Expected: i m full .\n",
      "Pred:  i m full . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, dataset, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(dataset[\"input_lang\"], sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        last_time_step_encoder_hidden_states = encoder.initHidden()\n",
    "        last_layer_encoder_hidden_states_foreach_input = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            last_layer_encoder_hidden_states, last_time_step_encoder_hidden_states = encoder(input_tensor[i], last_time_step_encoder_hidden_states)\n",
    "            last_layer_encoder_hidden_states_foreach_input[i] = last_layer_encoder_hidden_states.squeeze()\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = last_time_step_encoder_hidden_states\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, last_layer_encoder_hidden_states_foreach_input)\n",
    "            \n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset[\"output_lang\"].index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:i + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, dataset, n=3):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(dataset[\"pairs\"])\n",
    "        print(\"Input: \", pair[0])\n",
    "        print(\"Expected:\", pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, dataset, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print(\"Pred: \", output_sentence)\n",
    "        print('')\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bahdanau attention (also known as additive attention or concat attention) is defined as [follow](https://paperswithcode.com/method/additive-attention): $f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}_{j}\\right) = w_{a}^{T}\\tanh\\left(\\textbf{W}_{a}\\left[\\textbf{h}_{i};\\textbf{s}_{j}\\right]\\right)$ (1)\n",
    "- Sometimes we also see written as sum: $f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}_{j}\\right) = w_{a}^{T}\\tanh\\left(\\textbf{W}_{a}\\textbf{h}_{i} + \\textbf{U}_{a}\\textbf{s}_{j}\\right)$ (2)\n",
    "- This is because the projection (matmul) of 2 concatenated vectors <=> the sum of the projections of respective vectors ! ([source](https://stats.stackexchange.com/a/524729))\n",
    "    > - Note: the $\\textbf{W}_{a}$ in eq (1) and (2) are differents, it should be better to rewrite (2) as $f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}_{j}\\right) = w_{a}^{T}\\tanh\\left(\\textbf{T}_{a}\\textbf{h}_{i} + \\textbf{B}_{a}\\textbf{s}_{j}\\right)$ with $\\textbf{T}$ being the \"Top part\" and $\\textbf{B}$, the \"Bottom part\" of the same $\\textbf{W}$\n",
    "    > <p align=\"center\"> <img src=\"./assets/concat-add-bahdanau.png\" height=\"500\" width=\"900\" /></p>\n",
    "    \n",
    "    > - That's why they have different names (additive or concat attention)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- Summary: <p align=\"center\"> <img src=\"assets/part2-summary.png\" height=\"400\" width=\"700\" /></p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e26828b124e3e550b24680f350232b9e34674b0e62607605e5758cc17fa49831"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
